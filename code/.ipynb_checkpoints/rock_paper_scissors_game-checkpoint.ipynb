{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group project - Computational Game Theory\n",
    "\n",
    "# Playing rock paper scissors game\n",
    "\n",
    "In this game 2 agents compete with each other in a rock paper scissors game.\n",
    "Both players can heve their own learning algorithms assigned to make comparing different learners possible.\n",
    "This is done in an abstract way to make testing alternate situations easy.\n",
    "\n",
    "## TOC\n",
    "- Group info\n",
    "- Important note\n",
    "- Required imports\n",
    "- Storing and loading variables\n",
    "- Code to represent the players and games\n",
    "   - The WolfMatchingRPCPlayer class\n",
    "   - The PhcRPCPlayer class\n",
    "   - The RPCGame class\n",
    "   - The average_games function\n",
    "- Playing the game (WoLF)\n",
    "- Playing the game (HCP)\n",
    "- Plotting averages\n",
    "- Plotting single run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student info\n",
    "| Name     | Student number                        | Email address                               |\n",
    "| :---     | :---                          | :---                                |\n",
    "| Alexis Francois Verdoodt | _ | _ |\n",
    "| Lennert Bontinck | 568702 | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) |\n",
    "| Sofyan Ajridi | _ | _ |\n",
    "| Wolf De Wulf | 546395 | [wolf.de.wulf@vub.be](mailto:wolf.de.wulf@vub.be) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note\n",
    "In some codeblocks, the code might refer to variables from previous sections, in order to get accurate results code must be run top to bottom without skipping. The result of some lengthy processes is saved to a Pickle file to make the results available for reuse later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required imports\n",
    "All imports required for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import random\n",
    "import statistics as stats\n",
    "import math\n",
    "from scipy import stats as sstats\n",
    "\n",
    "# Allow for deep copying instead of python references (default)\n",
    "import copy\n",
    "\n",
    "# Plotting imports\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'svg'}\n",
    "\n",
    "# Library to save vars to files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and loading variables\n",
    "Improves reproducibility and allows for using previous results of lengthy processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_var_to_file(filename, network):\n",
    "    with open('savefiles/' + filename +'.pkl','wb') as f:\n",
    "        pickle.dump(network, f)\n",
    "\n",
    "def get_var_from_file(filename):\n",
    "    with open('savefiles/' + filename +'.pkl','rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "    \n",
    "#-------------------------\n",
    "\n",
    "#save to file example\n",
    "#save_var_to_file(\"folder/name\", var_to_save)\n",
    "    \n",
    "#open from file example\n",
    "#get_var_from_file = get_var_from_file(\"folder/name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to represent the players and games\n",
    "Let's start by making a way of representing the games and playing them in an abstract manner.\n",
    "\n",
    "### The WolfRPCPlayer class\n",
    "A class for defining a RPC player according to the WoLF algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WolfRPCPlayer:\n",
    "    \"\"\"\n",
    "    A class used to represent a matching pennies player using the WoLF algorithm.\n",
    "    Has no notion of state since there is only 1 state.\n",
    "    Makes use of a time-varrying alpha and delta thus it's not needed to specify these.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    q_values : list\n",
    "        List containing the player's current q values estimate for rock (0), paper (1) and scissors (1).\n",
    "    \n",
    "    policy_probabilities : list\n",
    "        List containing the player's current policy probabilities estimate for rock (0), paper (1) and scissors (1).\n",
    "    \n",
    "    average_policy_probabilities : list\n",
    "        List containing the player's current average policy probabilities estimate for rock (0), paper (1) and scissors (1).\n",
    "    \n",
    "    gamma : decimal\n",
    "        Decimal representing the discount factor.\n",
    "    \n",
    "    actions : list\n",
    "        List containing the three actions available to the player:  rock (0), paper (1) and scissors (1).\n",
    "    \n",
    "    state_count : int\n",
    "        Integer that represents the current state count. Since there is only 1 state this represents the iteration count.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    choose_action()\n",
    "        Chooses an action based on the current policy probabilities estimate for  rock (0), paper (1) and scissors (1).\n",
    "        There's a 0.05 probability of chosing a random action for exploration.\n",
    "        Returns the chosen action, which is thus represented as an integer.\n",
    "    \n",
    "    increment_state_count()\n",
    "        Increments the state count by one. Since there is only 1 state in this application this corresponds with the iteration count.\n",
    "    \n",
    "    update_q_value_for_action(action, received_reward)\n",
    "        Updates the estimated q-value using the new given reward.\n",
    "        \n",
    "    update_average_policy_probabilities(chosen_action, won)\n",
    "        Updates list containing the player's current average policy probabilities estimate for heads  rock (0), paper (1) and scissors (1).\n",
    "        Provide fixed first action and wether agent won (making averaging more useful).\n",
    "        \n",
    "    update_policy_probabilities(chosen_action, won)\n",
    "        Updates list containing the player's current policy probabilities estimate for  rock (0), paper (1) and scissors (1).\n",
    "        Provide fixed first action and wether agent won (making averaging more useful).\n",
    "    \n",
    "    Helper Methods\n",
    "    -------\n",
    "    player_is_winning()\n",
    "        Returns a bool specifying if a user is winning or not\n",
    "        \n",
    "    timed_alpha()\n",
    "        Returns the current alpha value to be used (for q-value calculations)\n",
    "        \n",
    "    timed_winning_delta()\n",
    "        Returns the current winning delta to be used for setting the average policy probabilities.\n",
    "        AKA winning learning rate        \n",
    "        \n",
    "    timed_losing_delta()\n",
    "        Returns the current losing delta to be used for setting the average policy probabilities.\n",
    "        AKA losing learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_q_value, initial_policy_probabilities, gamma):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_q_value : decimal\n",
    "            Decimal representing the initial value of the q_values.\n",
    "        initial_policy_probabilities : decimal\n",
    "            Decimal representing the initial value of the policy_probabilities and average_policy_probabilities.\n",
    "        gamma : decimal\n",
    "            Decimal representing alpha for updating gamma value estimations.\n",
    "        \"\"\"\n",
    "        self.q_values = [initial_q_value]*3\n",
    "        self.policy_probabilities = [initial_policy_probabilities]*3\n",
    "        self.average_policy_probabilities = [initial_policy_probabilities]*3\n",
    "        self.gamma = gamma\n",
    "        self.actions = [i for i in range(3)] # Rock (0) Paper(1) Scissor(2)\n",
    "        self.state_count = 0\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def choose_action(self):       \n",
    "        # give room for some exploration (e.g. stuck in an extreme)\n",
    "        if  random.random() < 0.05:\n",
    "            return random.choice(self.actions)\n",
    "        \n",
    "        # generate cumulative probabilities\n",
    "        cum_probs = np.cumsum(self.policy_probabilities)\n",
    "        \n",
    "        # chose an action for the cumulative probs\n",
    "        rnd = random.random()\n",
    "        for idx in range(len(cum_probs)):\n",
    "            if (rnd < cum_probs[idx]):\n",
    "                return idx\n",
    "        \n",
    "        \n",
    "    def increment_state_count(self):\n",
    "        self.state_count += 1\n",
    "        \n",
    "        \n",
    "    def update_q_value_for_action(self, action, received_reward):\n",
    "        crt_alpha = self.timed_alpha()\n",
    "        self.q_values[action] = ((1-crt_alpha) * self.q_values[action]) + crt_alpha * (received_reward + self.gamma * np.max(self.q_values))\n",
    "                \n",
    "        \n",
    "    def update_average_policy_probabilities(self, chosen_action, won):\n",
    "        c = self.state_count\n",
    "        \n",
    "        if c == 1: # just save first measure\n",
    "            for action in self.actions:\n",
    "                if won:\n",
    "                    self.average_policy_probabilities[action] = 1 if action == chosen_action else 0\n",
    "                else:\n",
    "                    self.average_policy_probabilities[action] = 1/2 if action != chosen_action else 0\n",
    "        else:\n",
    "            for action in self.actions:\n",
    "                polprob = self.policy_probabilities[action]\n",
    "                avgpolprob = self.average_policy_probabilities[action]\n",
    "                self.average_policy_probabilities[action] += (1/c)*(polprob-avgpolprob)\n",
    "    \n",
    "    \n",
    "    def update_policy_probabilities(self, chosen_action, won):\n",
    "        if self.state_count == 1: # just save first measure\n",
    "            for action in self.actions:\n",
    "                if won:\n",
    "                    self.average_policy_probabilities[action] = 1 if action == chosen_action else 0\n",
    "                else:\n",
    "                    self.average_policy_probabilities[action] = 1/2 if action != chosen_action else 0\n",
    "        else:\n",
    "            #determine delta\n",
    "            delta = self.timed_winning_delta() if self.player_is_winning() else self.timed_losing_delta()\n",
    "            \n",
    "            #determine 1 max q-value\n",
    "            max_action = np.argmax(self.q_values)\n",
    "            \n",
    "            for action in self.actions:\n",
    "                if action == max_action:\n",
    "                    self.policy_probabilities[action] = min(1.0,  self.policy_probabilities[action] + delta)\n",
    "                else: #devide losing over 2 from the 3 actions\n",
    "                    self.policy_probabilities[action] = max(0.0,  self.policy_probabilities[action] - (delta/2))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def player_is_winning(self):\n",
    "        policy_sum = 0\n",
    "        avg_policy_sum = 0\n",
    "        for action in self.actions:\n",
    "            policy_sum += self.policy_probabilities[action] * self.q_values[action]\n",
    "            avg_policy_sum += self.average_policy_probabilities[action] * self.q_values[action]\n",
    "            \n",
    "        return policy_sum > avg_policy_sum\n",
    "        \n",
    "    def timed_alpha(self):\n",
    "        return 1 / (100 + (self.state_count/10000))\n",
    "    \n",
    "    def timed_winning_delta(self): \n",
    "        return 1 / (20000 + self.state_count)\n",
    "    \n",
    "    def timed_losing_delta(self): \n",
    "        return 2 / (20000 + self.state_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PHCRPCPlayer class - TODO\n",
    "A class for defining a RPC player according to the PHC algorithm.\n",
    "Some of the WoLF specific functions will just be dummy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhcMatchingPenniesPlayer:\n",
    "    \"\"\"\n",
    "    A class used to represent a matching pennies player using the PHC algorithm.\n",
    "    Has no notion of state since there is only 1 state.\n",
    "    Has no notion of an average policy thus dummy functions will be used.\n",
    "    Makes use of a time-varrying alpha and delta thus it's not needed to specify these.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    q_values : list\n",
    "        List containing the player's current q values estimate for heads (0) and tail (1).\n",
    "    \n",
    "    policy_probabilities : list\n",
    "        List containing the player's current policy probabilities estimate for heads (0) and tail (1).\n",
    "    \n",
    "    average_policy_probabilities : list\n",
    "        Dummy functions.\n",
    "    \n",
    "    gamma : decimal\n",
    "        Decimal representing the discount factor.\n",
    "    \n",
    "    actions : list\n",
    "        List containing both actions available to the player: heads (0) and tail (1).\n",
    "    \n",
    "    delta : decimal\n",
    "        Decimal representing the delta if fixed delta is used\n",
    "    \n",
    "    alpha : decimal\n",
    "        Decimal representing the alpha if fixed alpha is used\n",
    "    \n",
    "    uses_time_varying : bool\n",
    "        Bool specifying whether or not time-varying alpha and delta is used.\n",
    "    \n",
    "    state_count : int\n",
    "        Integer that represents the current state count. Since there is only 1 state this represents the iteration count.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    choose_action()\n",
    "        Chooses an action based on the current policy probabilities estimate for heads (0) and tail (1).\n",
    "        There's a 0.05 probability of chosing a random action for exploration.\n",
    "        Returns the chosen action, which is thus represented as an integer.\n",
    "    \n",
    "    increment_state_count()\n",
    "        Increments the state count by one. Since there is only 1 state in this application this corresponds with the iteration count.\n",
    "    \n",
    "    update_q_value_for_action(action, received_reward)\n",
    "        Updates the estimated q-value using the new given reward.\n",
    "        \n",
    "    update_average_policy_probabilities(fixed_first_action)\n",
    "        Dummy function.\n",
    "        \n",
    "    update_policy_probabilities(fixed_first_action)\n",
    "        Updates list containing the player's current policy probabilities estimate for heads (0) and tail (1).\n",
    "        Provide fixed value for first value (making averaging more useful).\n",
    "    \n",
    "    Helper Methods\n",
    "    -------\n",
    "    timed_alpha()\n",
    "        Returns the current alpha value to be used (for q-value calculations)\n",
    "        \n",
    "    timed_delta()\n",
    "        Returns the current delta to be used for setting the estimated current policy probabilities.\n",
    "        AKA learning rate       \n",
    "    \"\"\"\n",
    "    def __init__(self, initial_q_value, initial_policy_probabilities, gamma,\n",
    "                 delta, alpha, uses_time_varying):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_q_value : decimal\n",
    "            Decimal representing the initial value of the q_values.\n",
    "        initial_policy_probabilities : decimal\n",
    "            Decimal representing the initial value of the policy_probabilities and average_policy_probabilities.\n",
    "        gamma : decimal\n",
    "            Decimal representing alpha for updating gamma value estimations.\n",
    "        \"\"\"\n",
    "        self.q_values = [initial_q_value]*2\n",
    "        self.policy_probabilities = [initial_policy_probabilities, initial_policy_probabilities]\n",
    "        self.average_policy_probabilities = [initial_policy_probabilities, initial_policy_probabilities]\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.alpha = alpha\n",
    "        self.uses_time_varying = uses_time_varying\n",
    "        self.actions = [i for i in range(2)]\n",
    "        self.state_count = 0\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def choose_action(self):       \n",
    "        # give room for some exploration (e.g. stuck in an extreme)\n",
    "        if  random.random() < 0.05:\n",
    "            return random.choice(self.actions)\n",
    "        \n",
    "        # generate cumulative probabilities\n",
    "        cum_probs = np.cumsum(self.policy_probabilities)\n",
    "        \n",
    "        # chose an action for the cumulative probs\n",
    "        rnd = random.random()\n",
    "        for idx in range(len(cum_probs)):\n",
    "            if (rnd < cum_probs[idx]):\n",
    "                return idx\n",
    "        \n",
    "        \n",
    "    def increment_state_count(self):\n",
    "        self.state_count += 1\n",
    "        \n",
    "        \n",
    "    def update_q_value_for_action(self, action, received_reward):\n",
    "        crt_alpha = self.timed_alpha() if self.uses_time_varying else self.alpha\n",
    "        self.q_values[action] = ((1-crt_alpha) * self.q_values[action]) + crt_alpha * (received_reward + self.gamma * np.max(self.q_values))\n",
    "                \n",
    "        \n",
    "    def update_average_policy_probabilities(self, fixed_first_action):\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def update_policy_probabilities(self, fixed_first_action):\n",
    "        if self.state_count == 1: # just save first measure\n",
    "            for action in self.actions:\n",
    "                self.policy_probabilities[action] = 1 if action == fixed_first_action else 0\n",
    "        else:\n",
    "            #determine delta\n",
    "            delta = self.timed_delta() if self.uses_time_varying else self.delta\n",
    "            \n",
    "            #determine 1 max q-value\n",
    "            max_action = np.argmax(self.q_values)\n",
    "            \n",
    "            for action in self.actions:\n",
    "                if action == max_action:\n",
    "                    self.policy_probabilities[action] = min(1.0,  self.policy_probabilities[action] + delta)\n",
    "                else:\n",
    "                    self.policy_probabilities[action] = max(0.0,  self.policy_probabilities[action] - delta)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def timed_alpha(self):\n",
    "        return 1 / (100 + (self.state_count/10000))\n",
    "    \n",
    "    def timed_delta(self): \n",
    "        return 1 / (20000 + self.state_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RPCGame class\n",
    "This game does't require the notion of states since the results are indepent of a given state. Thus the transition function can simply return the initial start state for every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPCGame:\n",
    "    \"\"\"\n",
    "    A class used to represent a RPC game.\n",
    "    Has no notion of state since there is only 1 state.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    players : list\n",
    "        List containing all (3) players for the game.\n",
    "        \n",
    "    \n",
    "    ----------\n",
    "    payoff_matrix : list\n",
    "        represents payoff matrix w.r.t. player 1.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    play_game(iterations)\n",
    "        Plays the game for the given amount of iterations.\n",
    "        Returns a list containing the policy probabilities for player 1 playing rock (0) and paper (1) over time.\n",
    "    \"\"\"\n",
    "    def __init__(self, players):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        players : list\n",
    "            List containing all (3) players for the game.\n",
    "        \"\"\"\n",
    "        self.players = players\n",
    "        self.payoff_matrix = [[0, -1, 1],\n",
    "                              [1, 0, -1],\n",
    "                              [-1, 1, 0]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def play_game(self, iterations):\n",
    "        results = [[0, 0] for _ in range(iterations)] #probs rock, probs paper from player 1\n",
    "        \n",
    "        # force initial value to make averaging more usefull\n",
    "        fixed_first_move_player_1 = 0 #rock\n",
    "        fixed_first_move_player_1_won = True\n",
    "        fixed_first_move_player_2 = 3 #scissor\n",
    "        fixed_first_move_player_2_won = False\n",
    "        \n",
    "        # determine player moves\n",
    "        for iteration in range(iterations):\n",
    "            player1_move = self.players[0].choose_action()\n",
    "            player2_move = self.players[1].choose_action()\n",
    "            \n",
    "            # add to state count\n",
    "            self.players[0].increment_state_count()\n",
    "            self.players[1].increment_state_count()\n",
    "            \n",
    "            # calculate corresponding reward and update q-value\n",
    "            reward = self.payoff_matrix[player1_move][player2_move]\n",
    "            self.players[0].update_q_value_for_action(player1_move, reward)\n",
    "            self.players[1].update_q_value_for_action(player2_move, -reward)\n",
    "            # update estimate of average policy\n",
    "            self.players[0].update_average_policy_probabilities(fixed_first_move_player_1, fixed_first_move_player_1_won)\n",
    "            self.players[1].update_average_policy_probabilities(fixed_first_move_player_2, fixed_first_move_player_2_won)\n",
    "            # update policy\n",
    "            self.players[0].update_policy_probabilities(fixed_first_move_player_1, fixed_first_move_player_1_won)\n",
    "            self.players[1].update_policy_probabilities(fixed_first_move_player_2, fixed_first_move_player_2_won)\n",
    "            # save result\n",
    "            results[iteration][0] = copy.deepcopy(self.players[0].policy_probabilities[0])\n",
    "            results[iteration][1] = copy.deepcopy(self.players[0].policy_probabilities[1])\n",
    "                \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average_games function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_games(game, iterations, trials):\n",
    "    results = [[0]*iterations for _ in range(trials)] \n",
    "    for trial in range(trials):\n",
    "        copied_game = copy.deepcopy(game)\n",
    "        results[trial] = copied_game.play_game(iterations)\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing the game (WoLF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ed2fa78a7167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRPCGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_games\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maveraging_amount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msave_var_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RPC/wolf_result\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ca0ee3fa961c>\u001b[0m in \u001b[0;36maverage_games\u001b[0;34m(game, iterations, trials)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcopied_game\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopied_game\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-14e83938a8f8>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(self, iterations)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# calculate corresponding reward and update q-value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayoff_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer1_move\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer2_move\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q_value_for_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer1_move\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q_value_for_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer2_move\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not NoneType"
     ]
    }
   ],
   "source": [
    "initial_q_value = 0\n",
    "initial_policy_probabilities = 1/3\n",
    "gamma = 0.8\n",
    "iterations = 1000000\n",
    "averaging_amount = 1\n",
    "\n",
    "player1 = WolfRPCPlayer(initial_q_value, initial_policy_probabilities, gamma)\n",
    "player2 = WolfRPCPlayer(initial_q_value, initial_policy_probabilities, gamma)\n",
    "players = [player1, player2]\n",
    "\n",
    "game = RPCGame(players)\n",
    "\n",
    "results = average_games(game, iterations, averaging_amount)\n",
    "save_var_to_file(\"RPC/wolf_result\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing the game (PHC | time-varying) - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_q_value = 0\n",
    "initial_policy_probabilities = 1/2\n",
    "gamma = 0.8\n",
    "delta = 0 #not used -> time-varying\n",
    "alpha = 0 #not used -> time-varying\n",
    "uses_time_varying = True\n",
    "iterations = 1000000\n",
    "averaging_amount = 30\n",
    "\n",
    "player1 = PhcMatchingPenniesPlayer(initial_q_value, initial_policy_probabilities, gamma, delta, alpha, uses_time_varying)\n",
    "player2 = PhcMatchingPenniesPlayer(initial_q_value, initial_policy_probabilities, gamma, delta, alpha, uses_time_varying)\n",
    "players = [player1, player2]\n",
    "\n",
    "game = MatchingPenniesGame(players)\n",
    "\n",
    "results = average_games(game, iterations, averaging_amount)\n",
    "save_var_to_file(\"matching_pennies/phc_result_time_varying\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing the game (PHC | fixed) - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_q_value = 0\n",
    "initial_policy_probabilities = 1/2\n",
    "gamma = 0.8\n",
    "delta = 1 / (20000 + 120000) #Can be finetuned further, this corresponds to the time-varying delta after 120'000 iterations\n",
    "alpha = 1 / (100 + (120000/10000)) #Can be finetuned further, this corresponds to the time-varying alpha after 120'000 iterations\n",
    "uses_time_varying = False\n",
    "iterations = 1000000\n",
    "averaging_amount = 30\n",
    "\n",
    "player1 = PhcMatchingPenniesPlayer(initial_q_value, initial_policy_probabilities, gamma, delta, alpha, uses_time_varying)\n",
    "player2 = PhcMatchingPenniesPlayer(initial_q_value, initial_policy_probabilities, gamma, delta, alpha, uses_time_varying)\n",
    "players = [player1, player2]\n",
    "\n",
    "game = MatchingPenniesGame(players)\n",
    "\n",
    "results = average_games(game, iterations, averaging_amount)\n",
    "save_var_to_file(\"matching_pennies/phc_result_fixed\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results\n",
    "results = [[\"WoLF-PHC\", get_var_from_file(\"matching_pennies/wolf_result\")]]\n",
    "           #[\"PHC with decay\", get_var_from_file(\"matching_pennies/phc_result_time_varying\")]\n",
    "           #[\"PHC\", get_var_from_file(\"matching_pennies/phc_result_fixed\")]]\n",
    "\n",
    "colors = [\"blue\",\n",
    "         \"orange\",\n",
    "         \"green\"]\n",
    "\n",
    "error_colors = [\"green\",\n",
    "                \"blue\",\n",
    "                \"orange\"]\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "color_idx = 0\n",
    "for result in results:\n",
    "    # average results over trials\n",
    "    result_data = result[1]\n",
    "    result_data = [np.array(x) for x in result_data]\n",
    "    average_result_data_over_all_trials = [np.mean(x) for x in zip(*result_data)]\n",
    "    \n",
    "    #determine iterations and averaging amount (trials)\n",
    "    iterations = len(average_result_data_over_all_trials)\n",
    "    averaging_amount = len(result_data)l\n",
    "    \n",
    "    # plot\n",
    "    x = np.arange(0, iterations, 1)\n",
    "    y = average_result_data_over_all_trials\n",
    "    plt.plot(x, y,\n",
    "             \"-\",\n",
    "             label=result[0],\n",
    "             color=colors[color_idx])\n",
    "    \n",
    "    color_idx += 1\n",
    "\n",
    "        \n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Pr(Heads)')\n",
    "plt.title('Probability of playing head over time\\n' + str(iterations) + \" iterations, \" + str(averaging_amount) + \" runs\")\n",
    "\n",
    "plt.savefig(\"graphs/matching_pennies/matching_pennies_all.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "color_idx = 0\n",
    "for result in results:\n",
    "    # average results over trials\n",
    "    result_data = result[1]\n",
    "    result_data = [np.array(x) for x in result_data]\n",
    "    average_result_data_over_all_trials = [np.mean(x) for x in zip(*result_data)]\n",
    "    \n",
    "    #determine iterations and averaging amount (trials)\n",
    "    iterations = len(average_result_data_over_all_trials)\n",
    "    averaging_amount = len(result_data)\n",
    "    \n",
    "    # plot\n",
    "    x = np.arange(0, iterations, 1)\n",
    "    y = average_result_data_over_all_trials\n",
    "    plt.plot(x, y,\n",
    "             \"-\",\n",
    "             label=result[0],\n",
    "             color=colors[color_idx])\n",
    "           \n",
    "    error = [sstats.sem(k) for k in zip(*result_data)]\n",
    "    plt.errorbar(x, y, yerr=error, errorevery=50000, color=colors[color_idx], alpha=1, ecolor=error_colors[color_idx], capsize=3, )\n",
    "    \n",
    "    color_idx += 1\n",
    "\n",
    "        \n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Pr(Heads)')\n",
    "plt.title('Probability of playing head over time\\n' + str(iterations) + \" iterations, \" + str(averaging_amount) + \" runs\")\n",
    "\n",
    "plt.savefig(\"graphs/matching_pennies/matching_pennies_all_with_error.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results\n",
    "results = [[\"WoLF-PHC\", get_var_from_file(\"matching_pennies/wolf_result\")],\n",
    "           [\"PHC with decay\", get_var_from_file(\"matching_pennies/phc_result_time_varying\")]]\n",
    "\n",
    "colors = [\"blue\",\n",
    "         \"orange\"]\n",
    "\n",
    "error_colors = [\"orange\",\n",
    "                \"blue\"]\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "color_idx = 0\n",
    "for result in results:\n",
    "    # average results over trials\n",
    "    result_data = result[1]\n",
    "    result_data = [np.array(x) for x in result_data]\n",
    "    average_result_data_over_all_trials = [np.mean(x) for x in zip(*result_data)]\n",
    "    \n",
    "    #determine iterations and averaging amount (trials)\n",
    "    iterations = len(average_result_data_over_all_trials)\n",
    "    averaging_amount = len(result_data)l\n",
    "    \n",
    "    # plot\n",
    "    x = np.arange(0, iterations, 1)\n",
    "    y = average_result_data_over_all_trials\n",
    "    plt.plot(x, y,\n",
    "             \"-\",\n",
    "             label=result[0],\n",
    "             color=colors[color_idx])\n",
    "    \n",
    "    color_idx += 1\n",
    "\n",
    "        \n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Pr(Heads)')\n",
    "plt.title('Probability of playing head over time\\n' + str(iterations) + \" iterations, \" + str(averaging_amount) + \" runs\")\n",
    "\n",
    "plt.savefig(\"graphs/matching_pennies/matching_pennies.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "color_idx = 0\n",
    "for result in results:\n",
    "    # average results over trials\n",
    "    result_data = result[1]\n",
    "    result_data = [np.array(x) for x in result_data]\n",
    "    average_result_data_over_all_trials = [np.mean(x) for x in zip(*result_data)]\n",
    "    \n",
    "    #determine iterations and averaging amount (trials)\n",
    "    iterations = len(average_result_data_over_all_trials)\n",
    "    averaging_amount = len(result_data)\n",
    "    \n",
    "    # plot\n",
    "    x = np.arange(0, iterations, 1)\n",
    "    y = average_result_data_over_all_trials\n",
    "    plt.plot(x, y,\n",
    "             \"-\",\n",
    "             label=result[0],\n",
    "             color=colors[color_idx])\n",
    "           \n",
    "    error = [sstats.sem(k) for k in zip(*result_data)]\n",
    "    plt.errorbar(x, y, yerr=error, errorevery=50000, color=colors[color_idx], alpha=1, ecolor=error_colors[color_idx], capsize=3, )\n",
    "    \n",
    "    color_idx += 1\n",
    "\n",
    "        \n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Pr(Heads)')\n",
    "plt.title('Probability of playing head over time\\n' + str(iterations) + \" iterations, \" + str(averaging_amount) + \" runs\")\n",
    "\n",
    "plt.savefig(\"graphs/matching_pennies/matching_pennies_with_error.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results\n",
    "results = [[\"WoLF-PHC\", get_var_from_file(\"matching_pennies/wolf_result\")]]\n",
    "           #[\"PHC with decay\", get_var_from_file(\"matching_pennies/phc_result_time_varying\")],\n",
    "           #[\"PHC\", get_var_from_file(\"matching_pennies/phc_result_fixed\")]]\n",
    "\n",
    "colors = [\"blue\",\n",
    "         \"orange\",\n",
    "         \"green\"]\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "color_idx = 0\n",
    "for result in results:\n",
    "    # average results over trials\n",
    "    result_data = result[1][0]\n",
    "    \n",
    "    #determine iterations\n",
    "    iterations = len(result_data)\n",
    "    \n",
    "    # plot\n",
    "    x = np.arange(0, iterations, 1)\n",
    "    y = result_data\n",
    "    plt.plot(x, y,\n",
    "             \"-\",\n",
    "             label=result[0],\n",
    "             color=colors[color_idx])\n",
    "    \n",
    "    color_idx += 1\n",
    "\n",
    "        \n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Pr(Heads)')\n",
    "plt.title('Probability of playing head over time\\n' + str(iterations) + \" iterations, single run\")\n",
    "\n",
    "plt.savefig(\"graphs/matching_pennies/matching_pennies_single_all.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results\n",
    "results = [[\"WoLF-PHC\", get_var_from_file(\"matching_pennies/wolf_result\")],\n",
    "           [\"PHC with decay\", get_var_from_file(\"matching_pennies/phc_result_time_varying\")]]\n",
    "\n",
    "colors = [\"blue\",\n",
    "         \"orange\"]\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "color_idx = 0\n",
    "for result in results:\n",
    "    # average results over trials\n",
    "    result_data = result[1][0]\n",
    "    \n",
    "    #determine iterations\n",
    "    iterations = len(result_data)\n",
    "    \n",
    "    # plot\n",
    "    x = np.arange(0, iterations, 1)\n",
    "    y = result_data\n",
    "    plt.plot(x, y,\n",
    "             \"-\",\n",
    "             label=result[0],\n",
    "             color=colors[color_idx])\n",
    "    \n",
    "    color_idx += 1\n",
    "\n",
    "        \n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Pr(Heads)')\n",
    "plt.title('Probability of playing head over time\\n' + str(iterations) + \" iterations, single run\")\n",
    "\n",
    "plt.savefig(\"graphs/matching_pennies/matching_pennies_single.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
