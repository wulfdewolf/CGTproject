{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group project - Computational Game Theory\n",
    "\n",
    "# Playing Matching pennies game\n",
    "\n",
    "In this game 2 agents compete with each other in a matching pennies game.\n",
    "Both players can heve their own learning algorithms assigned to make comparing different learners possible.\n",
    "This is done in an abstract way to make testing alternate situations easy.\n",
    "\n",
    "## TOC\n",
    "- Group info\n",
    "- Important note\n",
    "- Required imports\n",
    "- Storing and loading variables\n",
    "- Code to represent the players and games\n",
    "   - The MatchingPenniesPlayer class\n",
    "   - The MatchingPenniesGame class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student info\n",
    "| Name     | Student number                        | Email address                               |\n",
    "| :---     | :---                          | :---                                |\n",
    "| Alexis Francois Verdoodt | _ | _ |\n",
    "| Lennert Bontinck | 568702 | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) |\n",
    "| Sofyan Ajridi | _ | _ |\n",
    "| Wolf De Wulf | 546395 | [wolf.de.wulf@vub.be](mailto:wolf.de.wulf@vub.be) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note\n",
    "In some codeblocks, the code might refer to variables from previous sections, in order to get accurate results code must be run top to bottom without skipping. The result of some lengthy processes is saved to a Pickle file to make the results available for reuse later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required imports\n",
    "All imports required for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import random\n",
    "import statistics as stats\n",
    "import math\n",
    "from scipy import stats as sstats\n",
    "\n",
    "# Allow for deep copying instead of python references (default)\n",
    "import copy\n",
    "\n",
    "# Plotting imports\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'svg'}\n",
    "\n",
    "# Library to save vars to files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and loading variables\n",
    "Improves reproducibility and allows for using previous results of lengthy processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_var_to_file(filename, network):\n",
    "    with open('savefiles/' + filename +'.pkl','wb') as f:\n",
    "        pickle.dump(network, f)\n",
    "\n",
    "def get_var_from_file(filename):\n",
    "    with open('savefiles/' + filename +'.pkl','rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "    \n",
    "#-------------------------\n",
    "\n",
    "#save to file example\n",
    "#save_var_to_file(\"folder/name\", var_to_save)\n",
    "    \n",
    "#open from file example\n",
    "#get_var_from_file = get_var_from_file(\"folder/name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to represent the players and games\n",
    "Let's start by making a way of representing the games and playing them in an abstract manner.\n",
    "\n",
    "### The MatchingPenniesPlayer class\n",
    "A class for defining a matching pennies player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WolfMatchingPenniesPlayer:\n",
    "    \"\"\"\n",
    "    A class used to represent a matching pennies player.\n",
    "    Has no notion of state since there is only 1 state.\n",
    "    Makes use of a time-varrying alpha and delta thus it's not needed to specify these.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    q_values : list\n",
    "        List containing the player's current q values estimate for heads (0) and tail (1).\n",
    "    \n",
    "    policy_probabilities : list\n",
    "        List containing the player's current policy probabilities estimate for heads (0) and tail (1).\n",
    "    \n",
    "    average_policy_probabilities : list\n",
    "        List containing the player's current average policy probabilities estimate for heads (0) and tail (1).\n",
    "    \n",
    "    gamma : decimal\n",
    "        Decimal representing the discount factor.\n",
    "    \n",
    "    actions : list\n",
    "        List containing both actions available to the player: heads (0) and tail (1).\n",
    "    \n",
    "    state_count : int\n",
    "        Integer that represents the current state count. Since there is only 1 state this represents the iteration count.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    choose_action()\n",
    "        Chooses an action based on the current policy probabilities estimate for heads (0) and tail (1).\n",
    "        There's a 0.05 probability of chosing a random action for exploration.\n",
    "        Returns the chosen action, which is thus represented as an integer.\n",
    "    \n",
    "    increment_state_count()\n",
    "        Increments the state count by one. Since there is only 1 state in this application this corresponds with the iteration count.\n",
    "    \n",
    "    update_q_value_for_action(action, received_reward)\n",
    "        Updates the estimated q-value using the new given reward.\n",
    "        \n",
    "    update_average_policy_probabilities(fixed_first_action)\n",
    "        Updates list containing the player's current average policy probabilities estimate for heads (0) and tail (1).\n",
    "        Provide fixed value for first value (making averaging more useful).\n",
    "        \n",
    "    update_policy_probabilities(fixed_first_action)\n",
    "        Updates list containing the player's current policy probabilities estimate for heads (0) and tail (1).\n",
    "        Provide fixed value for first value (making averaging more useful).\n",
    "    \n",
    "    Helper Methods\n",
    "    -------\n",
    "    player_is_winning()\n",
    "        Returns a bool specifying if a user is winning or not\n",
    "        \n",
    "    timed_alpha()\n",
    "        Returns the current alpha value to be used\n",
    "        \n",
    "    timed_alpha()\n",
    "        Returns the current alpha value to be used (for q-value calculations)\n",
    "        \n",
    "    timed_winning_delta()\n",
    "        Returns the current winning delta to be used for setting the average policy probabilities.\n",
    "        AKA winning learning rate        \n",
    "        \n",
    "    timed_losing_delta()\n",
    "        Returns the current losing delta to be used for setting the average policy probabilities.\n",
    "        AKA losing learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_q_value, initial_policy_probabilities, gamma):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_q_value : decimal\n",
    "            Decimal representing the initial value of the q_values.\n",
    "        initial_policy_probabilities : decimal\n",
    "            Decimal representing the initial value of the policy_probabilities and average_policy_probabilities.\n",
    "        gamma : decimal\n",
    "            Decimal representing alpha for updating gamma value estimations.\n",
    "        \"\"\"\n",
    "        self.q_values = [initial_q_value]*2\n",
    "        self.policy_probabilities = [initial_policy_probabilities, initial_policy_probabilities]\n",
    "        self.average_policy_probabilities = [initial_policy_probabilities, initial_policy_probabilities]\n",
    "        self.gamma = gamma\n",
    "        self.actions = [i for i in range(2)]\n",
    "        self.state_count = 0\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def choose_action(self):       \n",
    "        # give room for some exploration (e.g. stuck in an extreme)\n",
    "        if  random.random() < 0.05:\n",
    "            return random.choice(self.actions)\n",
    "        \n",
    "        # generate cumulative probabilities\n",
    "        cum_probs = np.cumsum(self.policy_probabilities)\n",
    "        \n",
    "        # chose an action for the cumulative probs\n",
    "        rnd = random.random()\n",
    "        for idx in range(len(cum_probs)):\n",
    "            if (rnd < cum_probs[idx]):\n",
    "                return idx\n",
    "        \n",
    "        \n",
    "    def increment_state_count(self):\n",
    "        self.state_count += 1\n",
    "        \n",
    "        \n",
    "    def update_q_value_for_action(self, action, received_reward):\n",
    "        crt_alpha = self.timed_alpha()\n",
    "        self.q_values[action] = ((1-crt_alpha) * self.q_values[action]) + crt_alpha * (received_reward + self.gamma * np.max(self.q_values))\n",
    "                \n",
    "        \n",
    "    def update_average_policy_probabilities(self, fixed_first_action):\n",
    "        c = self.state_count\n",
    "        \n",
    "        if c == 1: # just save first measure\n",
    "            for action in self.actions:\n",
    "                self.average_policy_probabilities[action] = 1 if action == fixed_first_action else 0\n",
    "        else:\n",
    "            for action in self.actions:\n",
    "                polprob = self.policy_probabilities[action]\n",
    "                avgpolprob = self.average_policy_probabilities[action]\n",
    "                self.average_policy_probabilities[action] += (1/c)*(polprob-avgpolprob)\n",
    "    \n",
    "    \n",
    "    def update_policy_probabilities(self, fixed_first_action):\n",
    "        if self.state_count == 1: # just save first measure\n",
    "            for action in self.actions:\n",
    "                self.policy_probabilities[action] = 1 if action == fixed_first_action else 0\n",
    "        else:\n",
    "            #determine delta\n",
    "            delta = self.timed_winning_delta() if self.player_is_winning() else self.timed_losing_delta()\n",
    "            \n",
    "            #determine 1 max q-value\n",
    "            max_action = np.argmax(self.q_values)\n",
    "            \n",
    "            for action in self.actions:\n",
    "                if action == max_action:\n",
    "                    self.policy_probabilities[action] = min(1.0,  self.policy_probabilities[action] + delta)\n",
    "                else:\n",
    "                    self.policy_probabilities[action] = max(0.0,  self.policy_probabilities[action] - delta)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def player_is_winning(self):\n",
    "        policy_sum = 0\n",
    "        avg_policy_sum = 0\n",
    "        for action in self.actions:\n",
    "            policy_sum += self.policy_probabilities[action] * self.q_values[action]\n",
    "            avg_policy_sum += self.average_policy_probabilities[action] * self.q_values[action]\n",
    "            \n",
    "        return policy_sum > avg_policy_sum\n",
    "        \n",
    "    def timed_alpha(self):\n",
    "        return 1 / (100 + (self.state_count/10000))\n",
    "    \n",
    "    def timed_winning_delta(self): \n",
    "        return 1 / (20000 + self.state_count)\n",
    "    \n",
    "    def timed_losing_delta(self): \n",
    "        return 2 / (20000 + self.state_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MatchingPenniesGame class\n",
    "This game does't require the notion of states since the results are indepent of a given state. Thus the transition function can simply return the initial start state for every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchingPenniesGame:\n",
    "    \"\"\"\n",
    "    A class used to represent a matching pennies game.\n",
    "    Has no notion of state since there is only 1 state.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    players : list\n",
    "        List containing both (2) players for the game.\n",
    "        \n",
    "    \n",
    "    ----------\n",
    "    payoff_matrix : list\n",
    "        represents payoff matrix w.r.t. player 1.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    play_game(iterations, trials)\n",
    "        Plays the game for the given amount of iterations and trials.\n",
    "        Returns a list of lists with the first list representing all different games (trials) played.\n",
    "        For each game (trial) the policy probabilities for player 1 playing heads is saved (0).\n",
    "    \"\"\"\n",
    "    def __init__(self, players):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        players : list\n",
    "            List containing both (2) players for the game.\n",
    "        \"\"\"\n",
    "        self.players = players\n",
    "        self.payoff_matrix = [[1, -1], [-1,1]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def play_game(self, iterations, trials):\n",
    "        results = [[0]*iterations for _ in range(trials)]\n",
    "        \n",
    "        # force initial value to make averaging more usefull\n",
    "        player_1_result_1 = 0\n",
    "        player_2_result_1 = 1\n",
    "        \n",
    "        # determine player moves\n",
    "        for trial in range(trials):\n",
    "            for iteration in range(iterations):\n",
    "                player1_move = self.players[0].choose_action()\n",
    "                player2_move = self.players[1].choose_action()\n",
    "\n",
    "                # add to state count\n",
    "                self.players[0].increment_state_count()\n",
    "                self.players[1].increment_state_count()\n",
    "                \n",
    "                # calculate corresponding reward and update q-value\n",
    "                reward = self.payoff_matrix[player1_move][player2_move]\n",
    "                self.players[0].update_q_value_for_action(player1_move, reward)\n",
    "                self.players[1].update_q_value_for_action(player2_move, -reward)\n",
    "\n",
    "                # update estimate of average policy\n",
    "                self.players[0].update_average_policy_probabilities(player_1_result_1)\n",
    "                self.players[1].update_average_policy_probabilities(player_2_result_1)\n",
    "\n",
    "                # update policy\n",
    "                self.players[0].update_policy_probabilities(player_1_result_1)\n",
    "                self.players[1].update_policy_probabilities(player_2_result_1)\n",
    "\n",
    "                # save result\n",
    "                results[trial][iteration] = copy.deepcopy(self.players[0].policy_probabilities[0])\n",
    "                \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gamma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0f526d748280>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0maveraging_amount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplayer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWolfMatchingPenniesPlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_q_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_policy_probabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplayer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWolfMatchingPenniesPlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_q_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_policy_probabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gamma' is not defined"
     ]
    }
   ],
   "source": [
    "initial_q_value = 0\n",
    "initial_policy_probabilities = 1/2\n",
    "gamma = 0.8\n",
    "iterations = 1000000\n",
    "averaging_amount = 30\n",
    "\n",
    "player1 = WolfMatchingPenniesPlayer(initial_q_value, initial_policy_probabilities, gamma)\n",
    "player2 = WolfMatchingPenniesPlayer(initial_q_value, initial_policy_probabilities, gamma)\n",
    "players = [player1, player2]\n",
    "\n",
    "game = MatchingPenniesGame(players)\n",
    "\n",
    "results = game.play_game(iterations, averaging_amount)\n",
    "save_var_to_file(\"matching_pennies/wolf_result\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'savefiles/matching_pennies/wolf_result.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-130ba429eee0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"WoLF\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_var_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"matching_pennies/wolf_result\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m            \u001b[1;31m#[\"PHC\", get_var_from_file(\"matching_pennies/phc\")]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m colors = [\"blue\",\n",
      "\u001b[1;32m<ipython-input-2-3e6e58c7f5f2>\u001b[0m in \u001b[0;36mget_var_from_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_var_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'savefiles/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'savefiles/matching_pennies/wolf_result.pkl'"
     ]
    }
   ],
   "source": [
    "# get the results\n",
    "results = [[\"WoLF\", get_var_from_file(\"matching_pennies/wolf_result\")]]\n",
    "           #[\"PHC\", get_var_from_file(\"matching_pennies/phc\")]]\n",
    "\n",
    "colors = [\"blue\",\n",
    "         \"orange\"]\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "color_idx = 0\n",
    "for result in results:\n",
    "    # average results over trials\n",
    "    result_label = result[0]\n",
    "    result_data = result[1]\n",
    "    result_data = [np.array(x) for x in result_data]\n",
    "    average_result_data_over_all_trials = [np.mean(x) for x in zip(*result_data)]\n",
    "    \n",
    "    #determine iterations and averaging amount (trials)\n",
    "    iterations = len(average_result_data_over_all_trials)\n",
    "    averaging_amount = len(result_data)\n",
    "    \n",
    "    # plot\n",
    "    x = np.arange(0, iterations, 1)\n",
    "    y = average_result_data_over_all_trials\n",
    "    plt.plot(x, y,\n",
    "             \"-\",\n",
    "             label=result[0],\n",
    "             color=colors[color_idx])\n",
    "    \n",
    "    color_idx += 1\n",
    "\n",
    "        \n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Pr(Heads)')\n",
    "plt.title('Probability of playing head over time\\n' + str(iterations) + \" iterations, \" + str(averaging_amount) + \" runs\")\n",
    "\n",
    "plt.savefig(\"graphs/matching_pennies/matching_pennies.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "color_idx = 0\n",
    "for result in results:\n",
    "    # average results over trials\n",
    "    result_label = result[0]\n",
    "    result_data = result[1]\n",
    "    result_data = [np.array(x) for x in result_data]\n",
    "    average_result_data_over_all_trials = [np.mean(x) for x in zip(*result_data)]\n",
    "    \n",
    "    #determine iterations and averaging amount (trials)\n",
    "    iterations = len(average_result_data_over_all_trials)\n",
    "    averaging_amount = len(result_data)\n",
    "    \n",
    "    # plot\n",
    "    x = np.arange(0, iterations, 1)\n",
    "    y = average_result_data_over_all_trials\n",
    "    plt.plot(x, y,\n",
    "             \"-\",\n",
    "             label=result[0],\n",
    "             color=colors[color_idx])\n",
    "           \n",
    "    error = [sstats.sem(k) for k in zip(*result_data)]\n",
    "    plt.errorbar(x, y, yerr=error, errorevery=100000, color=colors[color_idx], capsize=3, )\n",
    "    \n",
    "    color_idx += 1\n",
    "\n",
    "        \n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Pr(Heads)')\n",
    "plt.title('Probability of playing head over time\\n' + str(iterations) + \" iterations, \" + str(averaging_amount) + \" runs\")\n",
    "\n",
    "plt.savefig(\"graphs/matching_pennies/matching_pennies_with_error.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
