{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group project - Computational Game Theory\n",
    "\n",
    "# Playing Matching pennies game\n",
    "\n",
    "In this game 2 agents compete with each other in a matching pennies game.\n",
    "Both players can heve their own learning algorithms assigned to make comparing different learners possible.\n",
    "This is done in an abstract way to make testing alternate situations easy.\n",
    "\n",
    "## TOC\n",
    "- Group info\n",
    "- Important note\n",
    "- Required imports\n",
    "- Storing and loading variables\n",
    "- Code to represent the players and games\n",
    "   - The MatchingPenniesPlayer class\n",
    "   - The MatchingPenniesGame class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student info\n",
    "| Name     | Student number                        | Email address                               |\n",
    "| :---     | :---                          | :---                                |\n",
    "| Alexis Francois Verdoodt | _ | _ |\n",
    "| Lennert Bontinck | 568702 | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) |\n",
    "| Sofyan Ajridi | _ | _ |\n",
    "| Wolf De Wulf | 546395 | [wolf.de.wulf@vub.be](mailto:wolf.de.wulf@vub.be) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note\n",
    "In some codeblocks, the code might refer to variables from previous sections, in order to get accurate results code must be run top to bottom without skipping. The result of some lengthy processes is saved to a Pickle file to make the results available for reuse later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import random\n",
    "import statistics as stats\n",
    "import math\n",
    "from scipy import stats as sstats\n",
    "\n",
    "# Allow for deep copying\n",
    "import copy\n",
    "\n",
    "# Plotting imports\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'svg'}\n",
    "\n",
    "# Library to save vars to files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and loading variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_var_to_file(filename, network):\n",
    "    with open('savefiles/' + filename +'.pkl','wb') as f:\n",
    "        pickle.dump(network, f)\n",
    "\n",
    "def get_var_from_file(filename):\n",
    "    with open('savefiles/' + filename +'.pkl','rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "    \n",
    "#-------------------------\n",
    "\n",
    "#save to file example\n",
    "#save_var_to_file(\"folder/name\", var_to_save)\n",
    "    \n",
    "#open from file example\n",
    "#get_var_from_file = get_var_from_file(\"folder/name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to represent the players and games\n",
    "Let's start by making a way of representing the games and playing them in an abstract manner.\n",
    "\n",
    "### The MatchingPenniesPlayer class\n",
    "A class for defining a matching pennies player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchingPenniesPlayer:\n",
    "    \"\"\"\n",
    "    A class used to represent a matching pennies player.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    xxx : xxx\n",
    "        xxx\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    xxx(yyy)\n",
    "        xxx\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_q_value, initial_policy_probabilities, alpha, gamma,\n",
    "                 winning_learning_rate, losing_learning_rate):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_q_value : decimal\n",
    "            Decimal representing the initial value of the q_values.\n",
    "        initial_policy_probabilities : decimal\n",
    "            Decimal representing the initial value of the policy_probabilities and average_policy_probabilities.\n",
    "        alpha : decimal\n",
    "            Decimal representing alpha for updating q value estimations.\n",
    "        gamma : decimal\n",
    "            Decimal representing alpha for updating gamma value estimations.\n",
    "        winning_learning_rate : decimal\n",
    "            decimal representing winning learning rate (delta).\n",
    "        losing_learning_rate : decimal\n",
    "            decimal representing losing learning rate (delta).\n",
    "        \"\"\"\n",
    "        self.q_values = [initial_q_value]*2\n",
    "        self.policy_probabilities = [initial_policy_probabilities, initial_policy_probabilities]\n",
    "        self.average_policy_probabilities = [initial_policy_probabilities, initial_policy_probabilities]\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.actions = [i for i in range(2)]\n",
    "        self.winning_delta = winning_learning_rate\n",
    "        self.losing_delta = losing_learning_rate\n",
    "        self.state_count = 0\n",
    "        \n",
    "\n",
    "    def update_q_value_for_action(self, action, received_reward):\n",
    "        crt_alpha = self.timed_alpha()\n",
    "        self.q_values[action] = ((1-crt_alpha) * self.q_values[action]) + crt_alpha * (received_reward + self.gamma * np.max(self.q_values))\n",
    "        \n",
    "    def choose_action(self):\n",
    "        cum_probs = np.cumsum(self.policy_probabilities)\n",
    "        \n",
    "        # some exploration\n",
    "        if  random.random() < 0.05:\n",
    "            return random.choice(self.actions)\n",
    "        \n",
    "        # chose an action for the cumulative probs\n",
    "        rnd = random.random()\n",
    "        for idx in range(len(cum_probs)):\n",
    "            if (rnd < cum_probs[idx]):\n",
    "                return idx\n",
    "                \n",
    "    def increment_state_count(self):\n",
    "        self.state_count += 1\n",
    "            \n",
    "    def update_average_policy_probabilities(self, chosen_action):\n",
    "        for action in self.actions:\n",
    "            c = self.state_count\n",
    "            polprob = self.policy_probabilities[action]\n",
    "            avgpolprob = self.average_policy_probabilities[action]\n",
    "            self.average_policy_probabilities[action] += (1/c)*(polprob-avgpolprob)\n",
    "\n",
    "    def player_is_winning(self):\n",
    "        policy_sum = 0\n",
    "        avg_policy_sum = 0\n",
    "        for action in self.actions:\n",
    "            policy_sum += self.policy_probabilities[action] * self.q_values[action]\n",
    "            avg_policy_sum += self.average_policy_probabilities[action] * self.q_values[action]\n",
    "            \n",
    "        return policy_sum > avg_policy_sum\n",
    "        \n",
    "    def timed_alpha(self):\n",
    "        return 1 / (100 + (self.state_count/10000))\n",
    "    \n",
    "    def timed_delta(self): \n",
    "        return 1 / (20000 + self.state_count)\n",
    "    \n",
    "    def update_policy_probabilities(self):\n",
    "        #determine delta\n",
    "        delta = self.winning_delta if self.player_is_winning() else self.losing_delta\n",
    "        \n",
    "        #determine 1 max q-value\n",
    "        max_action = np.argmax(self.q_values)\n",
    "        \n",
    "        for action in self.actions:\n",
    "            if action == max_action:\n",
    "                self.policy_probabilities[action] = min(1.0,  self.policy_probabilities[action] + self.timed_delta())\n",
    "            else:\n",
    "                self.policy_probabilities[action] = max(0.0,  self.policy_probabilities[action] - self.timed_delta())            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MatchingPenniesGame class\n",
    "This game does't require the notion of states since the results are indepent of a given state. Thus the transition function can simply return the initial start state for every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchingPenniesGame:\n",
    "    \"\"\"\n",
    "    A class used to represent a matching pennies game.\n",
    "    \"\"\"\n",
    "    def __init__(self, players):\n",
    "        self.players = players\n",
    "        self.payoff_matrix = [[1, -1], [-1,1]]\n",
    "        \n",
    "    def play_game(self, iterations, trials):\n",
    "        results = [[[0]*iterations, [0]*iterations] for _ in range(trials)]\n",
    "        \n",
    "        # determine player moves\n",
    "        for trial in range(trials):\n",
    "            for iteration in range(iterations):\n",
    "                player1_move = self.players[0].choose_action()\n",
    "                player2_move = self.players[1].choose_action()\n",
    "\n",
    "                # add to state count\n",
    "                self.players[0].increment_state_count()\n",
    "                self.players[1].increment_state_count()\n",
    "                \n",
    "                # calculate corresponding reward and update q-value\n",
    "                reward = self.payoff_matrix[player1_move][player2_move]\n",
    "                self.players[0].update_q_value_for_action(player1_move, reward)\n",
    "                self.players[1].update_q_value_for_action(player2_move, -reward)\n",
    "\n",
    "                # update estimate of average policy\n",
    "                self.players[0].update_average_policy_probabilities(player1_move)\n",
    "                self.players[1].update_average_policy_probabilities(player2_move)\n",
    "\n",
    "                # update policy\n",
    "                self.players[0].update_policy_probabilities()\n",
    "                self.players[1].update_policy_probabilities()\n",
    "\n",
    "                # save result player1\n",
    "                results[trial][0][iteration] = copy.deepcopy(self.players[0].policy_probabilities[0])\n",
    "                # save result player2\n",
    "                results[trial][1][iteration] = copy.deepcopy(self.players[1].policy_probabilities[1])\n",
    "                \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_q_value = 0\n",
    "initial_policy_probabilities = 1/2\n",
    "alpha = 0.2\n",
    "gamma = 0.8\n",
    "winning_learning_rate = 0.1\n",
    "losing_learning_rate = 0.2\n",
    "iterations = 1000000\n",
    "averaging_amount = 1\n",
    "\n",
    "player1 = MatchingPenniesPlayer(initial_q_value, initial_policy_probabilities, alpha, gamma,\n",
    "                                winning_learning_rate, losing_learning_rate)\n",
    "\n",
    "player2 = MatchingPenniesPlayer(initial_q_value, initial_policy_probabilities, alpha, gamma,\n",
    "                                winning_learning_rate, losing_learning_rate)\n",
    "\n",
    "players = [player1, player2]\n",
    "\n",
    "game = MatchingPenniesGame(players)\n",
    "\n",
    "results = game.play_game(iterations, averaging_amount)\n",
    "results\n",
    "save_var_to_file(\"matching_pennies/result\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results\n",
    "results = [[\"player1\", get_var_from_file(\"matching_pennies/result\")[0][0]]]\n",
    "\n",
    "colors = [\"blue\",\n",
    "         \"orange\",\n",
    "         \"green\",\n",
    "         \"red\",\n",
    "         \"purple\",\n",
    "         \"brown\"]\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "color_idx = 0\n",
    "for result in results:\n",
    "    x = np.arange(len(result[1]))\n",
    "    y = result[1]\n",
    "    plt.plot(x, y,\n",
    "             \"-\",\n",
    "             label=result[0],\n",
    "             color=colors[color_idx])\n",
    "           \n",
    "    color_idx += 1\n",
    "        \n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "#plt.xlabel('probability playing heads')\n",
    "#plt.ylabel('average reward')\n",
    "#plt.title('average reward over time\\n ' + str(iterations) + \" iterations, \" + str(averaging_amount) + \" runs\")\n",
    "\n",
    "#plt.savefig(\"graphs/todo\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
